{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Object Detection Evaluation Tutorial\n",
    "\n",
    "Welcome to the 3D object detection evaluation tutorial! We'll walk through the steps to submit your detections to the competition server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from av2.evaluation.detection.eval import evaluate\n",
    "from av2.evaluation.detection.utils import DetectionCfg\n",
    "from pathlib import Path\n",
    "from av2.utils.io import read_feather, read_all_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the evaluation configuration\n",
    "\n",
    "The `DetectionCfg` class stores the configuration for the 3D object detection challenge.\n",
    "\n",
    "- During evaluation, we remove _all_ cuboids which are not within the region-of-interest (ROI) which spatially is a 5 meter dilation of the drivable area isocontour. \n",
    "\n",
    "- **NOTE**: If you would like to _locally_ enable this behavior, you **must** pass in the directory to sensor dataset (to build the raster maps from the included vector maps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = Path.home() / \"data\" / \"datasets\" / \"av2\" / \"sensor\"  # Path to your AV2 sensor dataset directory.\n",
    "competition_cfg = DetectionCfg(dataset_dir=dataset_dir)  # Defaults to competition parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = \"val\"\n",
    "gts = read_all_annotations(dataset_dir=dataset_dir, split=split)  # Contains all annotations in a particular split.\n",
    "display(gts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing detections for evaluation.\n",
    "\n",
    "The evaluation expects the following 14 fields within a `pandas.DataFrame`:\n",
    "\n",
    "- `tx_m`: x-component of the object translation in the egovehicle reference frame.\n",
    "- `ty_m`: y-component of the object translation in the egovehicle reference frame.\n",
    "- `tz_m`: z-component of the object translation in the egovehicle reference frame.\n",
    "- `length_m`: Object extent along the x-axis in meters.\n",
    "- `width_m`: Object extent along the y-axis in meters.\n",
    "- `height_m`: Object extent along the z-axis in meters.\n",
    "- `qw`: Scalar quaternion coefficient.\n",
    "- `qx`: First quaternion coefficient.\n",
    "- `qy`: Second quaternion coefficient.\n",
    "- `qz`: Third quaternion coefficient.\n",
    "- `score`: Object confidence.\n",
    "- `log_id`: Log id associated with the detection.\n",
    "- `timestamp_ns`: Timestamp associated with the detection.\n",
    "- `category`: Object category.\n",
    "\n",
    "Example submission \n",
    "\n",
    "```python\n",
    "                tx_m       ty_m      tz_m  length_m   width_m  height_m        qw   qx   qy        qz     score                                log_id        timestamp_ns         category\n",
    "0        -162.932968   1.720428  0.039064  1.596262  0.772320  1.153996  0.125843  0.0  0.0  0.992050  0.127634  b0116f1c-f88f-3c09-b4bf-fc3c8ebeda56  315968193659921000       WHEELCHAIR\n",
    "1        -120.362213  19.875946 -0.382618  1.441901  0.593825  1.199819  0.802836  0.0  0.0  0.596200  0.126565  b0116f1c-f88f-3c09-b4bf-fc3c8ebeda56  315968193659921000          BICYCLE\n",
    "...\n",
    "14000000   10.182907  29.489899  0.662969  9.166531  1.761454  1.615999  0.023469  0.0  0.0 -0.999725  0.622177  b2d9d8a5-847b-3c3b-aed1-c414319d20af  315978610360111000  REGULAR_VEHICLE\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you've already aggregated your detections into one file.\n",
    "dts_path = Path(\"detections.feather\")\n",
    "dts = read_feather(dts_path)\n",
    "\n",
    "dts, gts, metrics = evaluate(dts, gts, cfg=competition_cfg)  # Evaluate instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you would like to submit to the evaluation server, you just need to export your detections into a `.feather` file. This can be done by:\n",
    "\n",
    "```python\n",
    "dts.to_feather(\"detections.feather\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b5ba47cd2ceb6b4b8c5d326eb375c1eb469e3d27b6f068302112bb12ba7862a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('av2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
